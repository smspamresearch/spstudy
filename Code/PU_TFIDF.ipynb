{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUdj2MU4SN3c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, os\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from numpy import quantile, where, random\n",
        "from baggingPU import BaggingClassifierPU\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def text_preprocess(mess):\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\n",
        "\n",
        "    nopunc = ''.join(nopunc)\n",
        "    nopunc = nopunc.lower()\n",
        "\n",
        "    nostop = [word for word in nopunc.split() if word.lower() not in stopwords.words('english') and word.isalpha()]\n",
        "    return nostop\n",
        "\n",
        "df_raw = pd.read_csv(\"super23.csv\", encoding='latin-1')\n",
        "# df_raw = pd.read_excel('punny.xlsx')\n",
        "# df_raw = df_raw.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
        "# df_raw = df_raw.drop(labels = [\"Unnamed: 2\"], axis = 1)\n",
        "df_raw.columns = [\"label\", \"text\"]\n",
        "# print (df_raw.head())\n",
        "\n",
        "messages_train = df_raw.loc[0:52588, :] \n",
        "# messages_test = df_raw.loc[55461:55685, :]\n",
        "\n",
        "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
        "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
        "    columnwidth = max([len(x) for x in labels]) + 4\n",
        "    empty_cell = \" \" * columnwidth\n",
        "    print(\"    \" + empty_cell, end=' ')\n",
        "    for label in labels:\n",
        "        print(\"%{0}s\".format(columnwidth) % 'pred_' + label, end=\" \")\n",
        "    print()\n",
        "\n",
        "    # Print rows\n",
        "    for i, label1 in enumerate(labels):\n",
        "        print(\"    %{0}s\".format(columnwidth) % 'true_' + label1, end=\" \")\n",
        "        for j in range(len(labels)):\n",
        "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
        "            if hide_zeroes:\n",
        "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
        "            if hide_diagonal:\n",
        "                cell = cell if i != j else empty_cell\n",
        "            if hide_threshold:\n",
        "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
        "            if cell:\n",
        "                print(cell, end=\" \")\n",
        "        print()\n",
        "\n",
        "def random_undersampling(tmp_df, TARGET_LABEL):\n",
        "    df_majority = tmp_df[tmp_df[TARGET_LABEL] == 0]\n",
        "    df_minority = tmp_df[tmp_df[TARGET_LABEL] == 1]\n",
        "\n",
        "    # Downsample majority class\n",
        "    df_majority_downsampled = resample(df_majority,\n",
        "                                       replace=False,              # sample without replacement\n",
        "                                       n_samples=len(df_minority), # to match minority class\n",
        "                                       random_state=None)        # reproducible results\n",
        "    # Combine minority class with downsampled majority class\n",
        "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "    print(\"Undersampling complete!\")\n",
        "    print(df_downsampled[TARGET_LABEL].value_counts())\n",
        "    return df_downsampled\n",
        "\n",
        "df_downsampled = random_undersampling(messages_train, 'label')\n",
        "df_downsampled = df_downsampled.sample(frac=1) #Shuffle the data\n",
        "df_downsampled = df_downsampled.reset_index() #Reset the index\n",
        "df_downsampled = df_downsampled.drop(columns=['index']) # Drop original index col\n",
        "\n",
        "df_downsampled.head()\n",
        "\n",
        "'''\n",
        "Lets make some negatives out of the positives by unlabeling a certain number of data points\n",
        "\n",
        "'''\n",
        "# Make a new df because we will need that for later\n",
        "df = df_downsampled.copy()\n",
        "\n",
        "#Separate cols from Train label\n",
        "NON_LBL = [c for c in df.columns if c != 'label']\n",
        "df[\"text\"] = df[NON_LBL]\n",
        "# print (X)\n",
        "YY = df[\"text\"]\n",
        "print (YY)\n",
        "df[\"text\"] = df[\"text\"].apply(text_preprocess)\n",
        "df[\"text\"] = df[\"text\"].agg(lambda x: ' '.join(map(str, x)))\n",
        "y = df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRsNDtrXeMD3",
        "outputId": "42dd7f84-842d-47a4-afa8-c897a8ea4577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train vector shape:  (29264, 191884)\n",
            "- 14632 positive out of 29264 total before hiding labels\n",
            "- 14032 positive out of 29264 total after hiding labels\n",
            "Training bagging classifier...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", ngram_range=(2, 2))\n",
        "X_messages_tfidf = vec.fit_transform(df[\"text\"]) #vec.fit_transform (train model) IS DIFFERENT THAN vec.transform\n",
        "print(\"Train vector shape: \",X_messages_tfidf.shape)\n",
        "\n",
        "# X_Test_messages_tfidf = vec.transform(messages_test[\"text\"])\n",
        "# print(\"Test vector shape: \", X_Test_messages_tfidf.shape)\n",
        "\n",
        "# Save the original labels and indices\n",
        "y_orig = y.copy()\n",
        "original_idx = np.where(df_downsampled.label == 1)\n",
        "\n",
        "# Here we are imputing 300 positives as negative\n",
        "hidden_size = 300\n",
        "y.loc[\n",
        "    np.random.choice(\n",
        "        y[y == 1].index,\n",
        "        replace = False,\n",
        "        size = hidden_size\n",
        "    )\n",
        "] = 0\n",
        "\n",
        "# Now we have 910 unreliable \"negatives\" and 310 true positives\n",
        "pd.Series(y).value_counts()\n",
        "\n",
        "# print('- %d samples and %d features' % (X.shape))\n",
        "print('- %d positive out of %d total before hiding labels' % (sum(df_downsampled.label), len(df_downsampled.label)))\n",
        "print('- %d positive out of %d total after hiding labels' % (sum(y), len(y)))\n",
        "\n",
        "print('Training bagging classifier...')\n",
        "pu_start = time.perf_counter()\n",
        "# Bagging with RandomForestClassifier\n",
        "bc = BaggingClassifierPU(RandomForestClassifier(n_estimators=20, random_state=2019),\n",
        "                         n_estimators = 50,\n",
        "                         n_jobs = -1,\n",
        "                         max_samples = sum(y)  # Each training sample will be balanced\n",
        "                        )\n",
        "# Bagging with SVM\n",
        "# svc = SVC(C=10, kernel='rbf', gamma=0.4, probability=True)\n",
        "# bc = BaggingClassifierPU(\n",
        "#     base_estimator=svc, n_estimators=15)\n",
        "\n",
        "bc.fit(X_messages_tfidf, y)\n",
        "pu_end = time.perf_counter()\n",
        "print('Done!')\n",
        "print('Time:', pu_end - pu_start)\n",
        "\n",
        "# print('---- {} ----'.format('PU Bagging'))\n",
        "# print(print_cm(sklearn.metrics.confusion_matrix(y_test, bc.predict(X_Test_messages_tfidf)), labels=['negative', 'positive']))\n",
        "# print('')\n",
        "# print('Precision: ', precision_score(y_test, bc.predict(X_Test_messages_tfidf)))\n",
        "# print('Recall: ', recall_score(y_test, bc.predict(X_Test_messages_tfidf)))\n",
        "# print('Accuracy: ', accuracy_score(y_test, bc.predict(X_Test_messages_tfidf)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4eZ2ut6obv6"
      },
      "outputs": [],
      "source": [
        "messages_test = df_raw.loc[52589:65736, :] # 55461:55685 55686:55910 55911:56135 56136:56360 56361:56585 56586:56810\n",
        "#Separate cols from Test labels\n",
        "NON_LBL_TEST = [c for c in messages_test.columns if c != 'label']\n",
        "messages_test[\"text\"] = messages_test[NON_LBL_TEST]\n",
        "print (messages_test[\"text\"])\n",
        "messages_test[\"text\"] = messages_test[\"text\"].apply(text_preprocess)\n",
        "messages_test[\"text\"] = messages_test[\"text\"].agg(lambda x: ' '.join(map(str, x)))\n",
        "y_test = messages_test['label']\n",
        "X_Test_messages_tfidf = vec.transform(messages_test[\"text\"])\n",
        "print(\"Test vector shape: \", X_Test_messages_tfidf.shape)\n",
        "print('---- {} ----'.format('PU Bagging'))\n",
        "test_pred = bc.predict(X_Test_messages_tfidf)\n",
        "# print(print_cm(sklearn.metrics.confusion_matrix(y_test, bc.predict(X_Test_messages_tfidf)), labels=['negative', 'positive']))\n",
        "print(print_cm(sklearn.metrics.confusion_matrix(y_test, test_pred), labels=['negative', 'positive']))\n",
        "\n",
        "print('')\n",
        "\n",
        "# print(metrics.classification_report(y_test, test_pred))\n",
        "print(\"precision: \", metrics.precision_score(y_test, test_pred))\n",
        "print(\"recall: \", metrics.recall_score(y_test, test_pred))\n",
        "print('Accuracy: ', accuracy_score(y_test, test_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}