{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK3h7DivKPal"
      },
      "outputs": [],
      "source": [
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!wget -c \"https://figshare.com/ndownloader/files/10798046\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuXzhQRcKcGm"
      },
      "outputs": [],
      "source": [
        "!gzip -d GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXwlMpNs4ceU"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-4KuoeM4gj0"
      },
      "outputs": [],
      "source": [
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzs19jgsIXoN"
      },
      "outputs": [],
      "source": [
        "#Fine Tune Glove\n",
        "# https://gist.github.com/chmodsss/867e01cc3eeeaa42226ac931709077dc\n",
        "# https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39\n",
        "# https://stackoverflow.com/questions/50909726/fine-tuning-glove-embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-_nIWtSQsvC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QtIYKM3kSlv"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, time, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, os\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from numpy import quantile, where, random\n",
        "import gensim\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.utils import resample\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "messages = pd.read_csv(\"super23.csv\", encoding='latin-1')\n",
        "# messages = pd.read_excel(\"punny.xlsx\")\n",
        "# messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\"], axis = 1)\n",
        "messages.columns = [\"label\",\"message\"]\n",
        "print (messages.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwF4LbK85hHT"
      },
      "outputs": [],
      "source": [
        "url = \"GoogleNews-vectors-negative300.bin\"\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_BXMaNe5m--"
      },
      "outputs": [],
      "source": [
        "def read_glove_vector(glove_vec):\n",
        "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
        "    words = set()\n",
        "    word_to_vec_map = {}\n",
        "    for line in f:\n",
        "      w_line = line.split()\n",
        "      curr_word = w_line[0]\n",
        "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
        "\n",
        "  return word_to_vec_map\n",
        "embeddings = read_glove_vector('glove.6B.100d.txt') #glove.6B.50d.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkfc-3LO5nlK"
      },
      "outputs": [],
      "source": [
        "print (embeddings[\"win\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij83eCfDN3V9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "# messages_train = messages.loc[0:55460, :]\n",
        "messages_train = messages.loc[37616:65736, :] #0:42627, 10658:53286, 0:52812\n",
        "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
        "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
        "    columnwidth = max([len(x) for x in labels]) + 4\n",
        "    empty_cell = \" \" * columnwidth\n",
        "    print(\"    \" + empty_cell, end=' ')\n",
        "    for label in labels:\n",
        "        print(\"%{0}s\".format(columnwidth) % 'pred_' + label, end=\" \")\n",
        "    print()\n",
        "\n",
        "    # Print rows\n",
        "    for i, label1 in enumerate(labels):\n",
        "        print(\"    %{0}s\".format(columnwidth) % 'true_' + label1, end=\" \")\n",
        "        for j in range(len(labels)):\n",
        "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
        "            if hide_zeroes:\n",
        "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
        "            if hide_diagonal:\n",
        "                cell = cell if i != j else empty_cell\n",
        "            if hide_threshold:\n",
        "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
        "            if cell:\n",
        "                print(cell, end=\" \")\n",
        "        print()\n",
        "\n",
        "def random_undersampling(tmp_df, TARGET_LABEL):\n",
        "    df_majority = tmp_df[tmp_df[TARGET_LABEL] == 1]\n",
        "    df_minority = tmp_df[tmp_df[TARGET_LABEL] == 0]\n",
        "\n",
        "    # Downsample majority class\n",
        "    df_majority_downsampled = resample(df_majority,\n",
        "                                       replace=False,              # sample without replacement\n",
        "                                       n_samples=len(df_minority), # to match minority class\n",
        "                                       random_state=None)        # reproducible results\n",
        "    # Combine minority class with downsampled majority class\n",
        "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "    print(\"Undersampling complete!\")\n",
        "    print(df_downsampled[TARGET_LABEL].value_counts())\n",
        "    return df_downsampled\n",
        "\n",
        "df_downsampled = random_undersampling(messages_train, 'label')\n",
        "df_downsampled = df_downsampled.sample(frac=1) #Shuffle the data\n",
        "df_downsampled = df_downsampled.reset_index() #Reset the index\n",
        "df_downsampled = df_downsampled.drop(columns=['index']) # Drop original index col\n",
        "\n",
        "df_downsampled.head()\n",
        "\n",
        "'''\n",
        "Lets make some negatives out of the positives by unlabeling a certain number of data points\n",
        "\n",
        "'''\n",
        "# Make a new df because we will need that for later\n",
        "df = df_downsampled.copy()\n",
        "\n",
        "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
        "stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
        "\n",
        "for doc in df['message'].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
        "    temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
        "    try:\n",
        "        for word in doc.split(' '): # looping through each word of a single document and spliting through space\n",
        "            if word not in stopwords: # if word is not present in stopwords then (try)\n",
        "                try:\n",
        "                    word_vec = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
        "                    temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
        "                except:\n",
        "                    pass\n",
        "        doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
        "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
        "    except:\n",
        "        pass\n",
        "print (docs_vectors.shape)\n",
        "stop = time.time()\n",
        "print(f\"Trg time: {stop - start}s\")  \n",
        "\n",
        "docs_vectors['label'] = df['label']\n",
        "print (docs_vectors.shape)\n",
        "docs_vectors = docs_vectors.replace(np.nan, 0) # All data frame\n",
        "# docs_vectors = docs_vectors.dropna()\n",
        "train_lbl = docs_vectors['label']\n",
        "print (train_lbl)\n",
        "\n",
        "# docs_vectors = docs_vectors.dropna()\n",
        "# train_lbl = train_lbl.dropna()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# model = SVC(gamma='scale', random_state=1)\n",
        "# model.fit(docs_vectors, train_lbl)\n",
        "# test_pred = model.predict(docs_vectors_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqIUODtJ9HNG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from baggingPU import BaggingClassifierPU\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print('Training bagging classifier...')\n",
        "pu_start = time.perf_counter()\n",
        "# Bagging with RandomForestClassifier\n",
        "bc = BaggingClassifierPU(RandomForestClassifier(n_estimators=20, random_state=3),\n",
        "                         n_estimators = 50,\n",
        "                         n_jobs = -1,\n",
        "                         max_samples = sum(train_lbl)  # Each training sample will be balanced\n",
        "                        )\n",
        "docs_vectors = docs_vectors.iloc[:,:-1]\n",
        "bc.fit(docs_vectors, train_lbl)\n",
        "pu_end = time.perf_counter()\n",
        "print('Done!')\n",
        "print('Trg Time:', pu_end - pu_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1DduyKHPimx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4809f9f8-fd4e-491f-e81f-03ef1c72cc6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(37616, 100)\n",
            "Testing time: 442.5656087398529s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "docs_vectors_test = pd.DataFrame() # creating empty final dataframe\n",
        "messages_test = messages.loc[0:37615, :] # 42627:53286, 0:10657, 52812:53286\n",
        "#For Test Data\n",
        "docs_vectors_test = pd.DataFrame() # creating empty final dataframe\n",
        "for doc in messages_test['message'].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
        "    temp1 = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
        "    try:\n",
        "        for word in doc.split(' '): # looping through each word of a single document and spliting through space\n",
        "            if word not in stopwords: # if word is not present in stopwords then (try)\n",
        "                try:\n",
        "                    word_vec1 = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
        "                    temp1 = temp1.append(pd.Series(word_vec1), ignore_index = True) # if word is present then append it to temporary dataframe\n",
        "                except:\n",
        "                    pass\n",
        "        doc_vector1 = temp1.mean() # take the average of each column(w0, w1, w2,........w300)\n",
        "        docs_vectors_test = docs_vectors_test.append(doc_vector1, ignore_index = True) # append each document value to the final dataframe\n",
        "    except:\n",
        "        pass\n",
        "print (docs_vectors_test.shape)\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcIVsdb92-xY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb8469d-c4a6-481e-aae3-aac8ca949ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label\n",
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      1\n"
          ]
        }
      ],
      "source": [
        "messages_test['label'].to_csv(\"lblpu212.csv\", index = False)\n",
        "ml = pd.read_csv(\"lblpu212.csv\", encoding='latin-1')\n",
        "ml.columns = [\"label\"]\n",
        "print (ml.head())\n",
        "# messages = pd.read_excel(\"punny.xlsx\")\n",
        "# messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\"], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hY4zRvdFjvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c98b981-55f6-4c4c-d8cb-a57d5c6a3f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- PU Bagging ----\n",
            "\n",
            "Testing time: 8.610342979431152s\n",
            "[[33842  2703]\n",
            " [  137   553]]\n",
            "Precision:  0.16984029484029484\n",
            "Recall:  0.8014492753623188\n",
            "Accuracy:  0.9237276755740567\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "test_lbl = ml['label']\n",
        "docs_vectors_test['label'] = ml['label']\n",
        "docs_vectors_test = docs_vectors_test.dropna()\n",
        "\n",
        "print('---- {} ----'.format('PU Bagging'))\n",
        "print('')\n",
        "\n",
        "# docs_vectors_test = docs_vectors_test.iloc[:,:-1]\n",
        "test_pred = bc.predict(docs_vectors_test.drop('label', axis = 1))\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\") \n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "print(confusion_matrix(docs_vectors_test['label'], test_pred))\n",
        "print(\"Precision: \", precision_score(docs_vectors_test['label'], test_pred))\n",
        "print(\"Recall: \", recall_score(docs_vectors_test['label'], test_pred))\n",
        "print (\"Accuracy: \", accuracy_score(docs_vectors_test['label'], test_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}