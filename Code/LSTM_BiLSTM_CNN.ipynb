{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3_2Jl4OgKHF",
        "outputId": "33eb3358-5b06-4095-ac8d-257c2370c213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kqhOS4GS4cm",
        "outputId": "4247fd5e-5558-4787-df54-41ea0961ce91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                            message\n",
            "0      1  YOU HAVE WON! As a valued Vodafone customer ou...\n",
            "1      1  Guinness Record! World?s most pierced women! C...\n",
            "2      1  U have a secret admirer. REVEAL who thinks U R...\n",
            "3      1  U have a secret admirer who is looking 2 make ...\n",
            "4      1  FREE for 1st week! No1 Nokia tone 4 ur mob eve...\n"
          ]
        }
      ],
      "source": [
        "# import libraries for reading data, exploring and plotting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "%matplotlib inline\n",
        "# library for train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# deep learning libraries for text pre-processing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Modeling \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "def text_preprocess(mess):\n",
        "\n",
        "    # Check characters to see if they are in punctuation\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\n",
        "\n",
        "    # Join the characters again to form the string.\n",
        "    nopunc = ''.join(nopunc)\n",
        "    nopunc = nopunc.lower()\n",
        "\n",
        "    # Now just remove any stopwords and non alphabets\n",
        "    nostop = [word for word in nopunc.split() if word.lower() not in stopwords.words('english') and word.isalpha()]\n",
        "    return nostop\n",
        "\n",
        "messages = pd.read_csv(\"super23.csv\", encoding='latin-1')\n",
        "# messages = pd.read_excel('punny.xlsx')\n",
        "messages.columns =[\"label\", \"message\"]\n",
        "\n",
        "print(messages.head(5))\n",
        "# print(messages.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsLl9yJQdZY8",
        "outputId": "38bd7027-8c17-4e47-cd8c-ec388bf4f86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       label                                            message\n",
            "65733      0             Are u struggling to access some apps. \n",
            "65734      0  Hey dude, could u please put my leave tomorrow...\n",
            "65735      0  hope u received my email, you can put ur own c...\n"
          ]
        }
      ],
      "source": [
        "messages_train = messages.loc[0:65736, :]\n",
        "print (messages_train.tail(3))\n",
        "\n",
        "messages_train[\"message\"] = messages_train[\"message\"].apply(text_preprocess)\n",
        "messages_train[\"message\"] = messages_train[\"message\"].agg(lambda x: ' '.join(map(str, x)))\n",
        "# train_msg = messages_train[\"message\"]\n",
        "msg_label = messages_train[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR051tyYW8ye",
        "outputId": "0849fb40-2607-4dd9-84a5-6a84d4935eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 55976 unique tokens in training data. \n",
            "Shape of training tensor:  (52588, 50)\n",
            "Shape of testing tensor:  (13148, 50)\n",
            "Before Padding:  8 13\n",
            "After Padding:  50 50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Split data into train and test\n",
        "train_msg, test_msg, train_labels, test_labels = train_test_split(messages_train[\"message\"], msg_label, test_size=0.2, random_state=434)\n",
        "\n",
        "# Defining pre-processing hyperparameters\n",
        "max_len = 50 \n",
        "trunc_type = \"post\" \n",
        "padding_type = \"post\" \n",
        "oov_tok = \"<OOV>\" \n",
        "vocab_size = 500\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, char_level=False, oov_token = oov_tok)\n",
        "tokenizer.fit_on_texts(train_msg)\n",
        "\n",
        "# Get the word_index \n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# check how many words \n",
        "tot_words = len(word_index)\n",
        "print('There are %s unique tokens in training data. ' % tot_words)\n",
        "\n",
        "# Sequencing and padding on training and testing \n",
        "training_sequences = tokenizer.texts_to_sequences(train_msg)\n",
        "training_padded = pad_sequences (training_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type )\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_msg)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)\n",
        "\n",
        "# Shape of train tensor\n",
        "print('Shape of training tensor: ', training_padded.shape)\n",
        "print('Shape of testing tensor: ', testing_padded.shape)\n",
        "\n",
        "# Before padding\n",
        "print(\"Before Padding: \", len(training_sequences[0]), len(training_sequences[1]))\n",
        "\n",
        "# After padding\n",
        "print(\"After Padding: \",len(training_padded[0]), len(training_padded[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqhozRn-GVPo"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 14000 # As defined earlier\n",
        "embeding_dim = 16\n",
        "drop_value = 0.2 # dropout"
      ],
      "metadata": {
        "id": "3Mk4oU6d13Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJZwXZ7CC5uE"
      },
      "source": [
        "* **LSTM MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkSERRLGgdVQ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Flatten\n",
        "#LSTM hyperparameters\n",
        "n_lstm = 140\n",
        "drop_lstm =0.5\n",
        "\n",
        "#LSTM Spam detection architecture\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, embeding_dim, input_length=max_len))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True))\n",
        "model1.add(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "\n",
        "num_epochs = 50\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history = model1.fit(training_padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels),callbacks =[early_stop], verbose=0)\n",
        "\n",
        "# Model performance on test data \n",
        "result = model1.evaluate(testing_padded, test_labels)\n",
        "\n",
        "loss, accuracy = model1.evaluate(testing_padded, test_labels)\n",
        "# loss, accuracy, f1_score, precision, recall = model1.evaluate(testing_padded, test_labels)\n",
        "print (\"Result: \", result)\n",
        "print (\"loss, accuracy: \", loss, accuracy)\n",
        "\n",
        "# # Create a dataframe\n",
        "# metrics = pd.DataFrame(history.history)\n",
        "# # Rename column\n",
        "# metrics.rename(columns = {'loss': 'Training_Loss', 'accuracy': 'Training_Accuracy',\n",
        "#                          'val_loss': 'Validation_Loss', 'val_accuracy': 'Validation_Accuracy'}, inplace = True)\n",
        "# def plot_graphs1(var1, var2, string):\n",
        "#     metrics[[var1, var2]].plot()\n",
        "#     plt.title('LSTM Model: Training and Validation ' + string)\n",
        "#     plt.xlabel ('Number of epochs')\n",
        "#     plt.ylabel(string)\n",
        "#     plt.legend([var1, var2])\n",
        "# plot_graphs1('Training_Loss', 'Validation_Loss', 'loss')\n",
        "# plot_graphs1('Training_Accuracy', 'Validation_Accuracy', 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lbl = (model1.predict(testing_padded) > 0.5).astype(\"int32\") #decode(en, x_test)\n",
        "pd.DataFrame(pred_lbl).to_csv('output1.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"output1.csv\", encoding='latin-1')\n",
        "messages.columns = [\"labels\"]\n",
        "print (messages.tail(3))"
      ],
      "metadata": {
        "id": "RVnQA7j92IC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(test_labels, messages[\"labels\"]))\n",
        "print(metrics.confusion_matrix(test_labels, messages[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(test_labels, messages[\"labels\"])))"
      ],
      "metadata": {
        "id": "G8I4kr4j2QOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1Z_vZXWCtSS"
      },
      "source": [
        "\n",
        "* **Bidirectional LSTM MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USsW11iLjBIc"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Flatten\n",
        "#LSTM hyperparameters\n",
        "n_lstm = 140\n",
        "drop_lstm =0.5\n",
        "\n",
        "# Biderectional LSTM Spam detection architecture\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size, embeding_dim, input_length=max_len))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True)))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "num_epochs = 10\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history = model2.fit(training_padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels),callbacks =[early_stop], verbose=2)\n",
        "\n",
        "# Model performance on test data \n",
        "result = model2.evaluate(testing_padded, test_labels)\n",
        "\n",
        "# loss, accuracy, f1_score, precision, recall = model.evaluate(testing_padded, test_labels)\n",
        "print (\"Result: \", result)\n",
        "# print (\"loss, accuracy, f1_score, precision, recall: \", loss, accuracy, f1_score, precision, recall)\n",
        "\n",
        "# # Create a dataframe\n",
        "# metrics = pd.DataFrame(history.history)\n",
        "# # Rename column\n",
        "# metrics.rename(columns = {'loss': 'Training_Loss', 'accuracy': 'Training_Accuracy',\n",
        "#                          'val_loss': 'Validation_Loss', 'val_accuracy': 'Validation_Accuracy'}, inplace = True)\n",
        "# def plot_graphs1(var1, var2, string):\n",
        "#     metrics[[var1, var2]].plot()\n",
        "#     plt.title('BiLSTM Model: Training and Validation ' + string)\n",
        "#     plt.xlabel ('Number of epochs')\n",
        "#     plt.ylabel(string)\n",
        "#     plt.legend([var1, var2])\n",
        "# # Plot\n",
        "# plot_graphs1('Training_Loss', 'Validation_Loss', 'loss')\n",
        "# plot_graphs1('Training_Accuracy', 'Validation_Accuracy', 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lbl2 = (model2.predict(testing_padded) > 0.5).astype(\"int32\") #decode(en, x_test)\n",
        "pd.DataFrame(pred_lbl2).to_csv('output2.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "messages2 = pd.read_csv(\"output2.csv\", encoding='latin-1')\n",
        "messages2.columns = [\"labels\"]\n",
        "print (messages2.tail(3))"
      ],
      "metadata": {
        "id": "tVEpuy6-BU3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(test_labels, messages2[\"labels\"]))\n",
        "print(metrics.confusion_matrix(test_labels, messages2[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(test_labels, messages2[\"labels\"])))"
      ],
      "metadata": {
        "id": "yFoZBeV5Bkt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmMr5P-qDcGd"
      },
      "source": [
        "**CNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOy03B16DbXu"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "embedding_size = 100\n",
        "\n",
        "model22 = Sequential()\n",
        "model22.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n",
        "model22.add(Conv1D(128, 3, activation='relu'))\n",
        "model22.add(MaxPool1D(3))\n",
        "model22.add(Dropout(0.2))\n",
        "model22.add(Conv1D(128, 3, activation='relu'))\n",
        "model22.add(GlobalMaxPooling1D())\n",
        "model22.add(Dropout(0.2))\n",
        "model22.add(Dense(64, activation='relu'))\n",
        "model22.add(Dropout(0.2))\n",
        "model22.add(Dense(32, activation='relu'))\n",
        "model22.add(Dropout(0.2))\n",
        "model22.summary()\n",
        "model22.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model22.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "num_epochs = 10\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "result22 = model22.fit(training_padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels),callbacks =[early_stop], verbose=2)\n",
        "\n",
        "# Model performance on test data \n",
        "result = model22.evaluate(testing_padded, test_labels)\n",
        "\n",
        "# loss, accuracy, f1_score, precision, recall = model.evaluate(testing_padded, test_labels)\n",
        "print (\"Result: \", result)\n",
        "# print (\"loss, accuracy, f1_score, precision, recall: \", loss, accuracy, f1_score, precision, recall)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lbl2 = (model22.predict(testing_padded) > 0.5).astype(\"int32\") #decode(en, x_test)\n",
        "pd.DataFrame(pred_lbl2).to_csv('output3.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "messages2 = pd.read_csv(\"output3.csv\", encoding='latin-1')\n",
        "messages2.columns = [\"labels\"]\n",
        "print (messages2.tail(3))"
      ],
      "metadata": {
        "id": "z9doi5a1Bvo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(test_labels, messages2[\"labels\"]))\n",
        "print(metrics.confusion_matrix(test_labels, messages2[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(test_labels, messages2[\"labels\"])))"
      ],
      "metadata": {
        "id": "5y4R1JgAB5Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TCHwMg4CiZe"
      },
      "source": [
        "* **CNN with Glove Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyxoReNjrKfF"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPj7xM1wrNpI"
      },
      "outputs": [],
      "source": [
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuk2IZpbp5T_"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_size = 100\n",
        "\n",
        "word2vec = {}\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "    \n",
        "print(len(word2vec))\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
        "\n",
        "for word,i in  tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = word2vec.get(word)\n",
        "        if embedding_vector is not None:\n",
        "         # words not in the glove will be set to zero   \n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "n_epochs = 50\n",
        "model4 = Sequential()\n",
        "model4.add(Embedding(input_dim=vocab_size, output_dim=embedding_size,weights = [embedding_matrix], input_length=max_len, trainable=True))\n",
        "model4.add(Conv1D(128, 3, activation='relu'))\n",
        "model4.add(MaxPool1D(3))\n",
        "model4.add(Dropout(0.2))\n",
        "model4.add(Conv1D(128, 3, activation='relu'))\n",
        "model4.add(GlobalMaxPooling1D())\n",
        "model4.add(Dropout(0.2))\n",
        "model4.add(Dense(64, activation='relu'))\n",
        "model4.add(Dropout(0.2))\n",
        "model4.add(Dense(32, activation='relu'))\n",
        "model4.add(Dropout(0.2))\n",
        "model4.add(Dense(16, activation='relu'))\n",
        "model4.add(Dropout(0.2))\n",
        "model4.summary()\n",
        "model4.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model4.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "save_best = ModelCheckpoint('SMS.hdf', save_best_only=True, monitor='val_acc', mode='max')\n",
        "result4 = model4.fit(training_padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels),callbacks =[early_stop], verbose=2)\n",
        "# result4 = model4.fit(np.array(X_train), np.array(y_train), batch_size = batch_size, epochs=n_epochs, validation_split=0.2, verbose=1,  callbacks=[save_best])\n",
        "eval_ = model4.evaluate(np.array(testing_padded), np.array(test_labels))\n",
        "print(eval_[0], eval_[1]) # loss / accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lbl2 = (model4.predict(testing_padded) > 0.5).astype(\"int32\") #decode(en, x_test)\n",
        "pd.DataFrame(pred_lbl2).to_csv('output4.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "messages2 = pd.read_csv(\"output4.csv\", encoding='latin-1')\n",
        "messages2.columns = [\"labels\"]\n",
        "print (messages2.tail(3))"
      ],
      "metadata": {
        "id": "eBHy5q3gPMBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(test_labels, messages2[\"labels\"]))\n",
        "print(metrics.confusion_matrix(test_labels, messages2[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(test_labels, messages2[\"labels\"])))"
      ],
      "metadata": {
        "id": "6M0-BRjaPfPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haj3tPCel3L0"
      },
      "source": [
        "other articles\n",
        "https://towardsdatascience.com/spam-detection-in-emails-de0398ea3b48\n",
        "https://towardsdatascience.com/deep-learning-techniques-for-text-classification-78d9dc40bf7c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKoiieVct6wv"
      },
      "source": [
        "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}