{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFOPitOY5y6J"
      },
      "source": [
        "# TCN Classification with Super Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQsVvtoN5y6S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU')) #\"/GPU:0\": The first GPU of machine that is visible to TensorFlow.\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# nltk.download('twitter_samples')\n",
        "print (\"Initiated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVJVx6iO5y6X"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoifQOZX5y6X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "corpus = pd.read_csv(\"super23_train.csv\", encoding='latin-1')\n",
        "corpus.columns =[\"sentence\",\"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hJevl9d5y6b",
        "scrolled": true,
        "outputId": "69da1813-cc5d-449a-f718-f7835ad75907"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'YOU HAVE WON! As a valued Vodafone customer our computer has picked YOU to win a ?150 prize. To collect is easy. Just call 09061743386'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2b0APNv5y6d"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbSc43yK5y6e"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "# print('Into a padded sequence:', training_padded[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2nu_M035y6f"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(\"Vocab Size: \",vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQbzGoQN5y6g"
      },
      "source": [
        "# Model 1: Embedding Random\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DZJOo2x9t3W"
      },
      "outputs": [],
      "source": [
        "# !pip install keras-tcn==3.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nFUGfV95y6i"
      },
      "outputs": [],
      "source": [
        "from tcn import TCN, tcn_full_summary\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.layers import concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def define_model(kernel_size = 3, activation='relu', input_dim = None, output_dim=300, max_length = None ):\n",
        "    \n",
        "    inp = Input( shape=(max_length,))\n",
        "    x = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_length)(inp)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    \n",
        "    x = TCN(128,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn1')(x)\n",
        "    x = TCN(64,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn2')(x)\n",
        "    \n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    \n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(16, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
        "\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_VkMHZi5y6j"
      },
      "outputs": [],
      "source": [
        "model_0 = define_model(input_dim=1000, max_length=100)\n",
        "print(\"Summary: \")\n",
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ku_VMkM5y6j",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# tcn_full_summary(model_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCIUywbT5y6k"
      },
      "outputs": [],
      "source": [
        "# class myCallback(tf.keras.callbacks.Callback):\n",
        "#     # Overide the method on_epoch_end() for our benefit\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         if (logs.get('accuracy') > 0.93):\n",
        "#             print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "#             self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=10, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsBksWr25y6k"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjt-HDm95y6k",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "activation = \"relu\"\n",
        "kernel_size = [3]\n",
        "\n",
        "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'AVG']\n",
        "record = pd.DataFrame(columns = columns)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "exp = 0\n",
        "train_x = sentences\n",
        "train_y = labels\n",
        "# Turn the labels into a numpy array\n",
        "train_y = np.array(train_y)\n",
        "# encode data using\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "# Define the input shape\n",
        "model = define_model(kernel_size, activation, input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model.fit(Xtrain, train_y, batch_size=16, epochs=10, verbose=1, callbacks=[callbacks])\n",
        "stop = time.time()\n",
        "print(f\"Training time per fold: {stop - start}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30s2VMnR5y6l"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ApjlNxPAr3S",
        "outputId": "d9183b82-a054-48c9-bf90-77bf2b71267c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13148, 2)\n",
            "download whichapp for whatsapp friends to see your friends apps and also save battery by  you have  friend waiting httpbitlyogmdkv\n",
            "Messages length 13148\n"
          ]
        }
      ],
      "source": [
        "corpus1 = pd.read_csv(\"super23_test.csv\", encoding='latin-1')\n",
        "# corpus1 = pd.read_excel(\"test_punny.xlsx\")\n",
        "corpus1.columns =[\"sentence\", \"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus1.label = corpus1.label.astype(int)\n",
        "print(corpus1.shape)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "print(sentences1[0])\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "\n",
        "test_x1 = sentences1\n",
        "test_y1 = labels1\n",
        "\n",
        "test_sequences1 = tokenizer.texts_to_sequences(test_x1)\n",
        "Xtest1 = pad_sequences(test_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(\"Messages length\", len(test_x1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPpPdG44Ar3S",
        "outputId": "7424cc47-0ee5-41e3-adfb-ab23773febbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "411/411 [==============================] - 5s 10ms/step\n",
            "classification time: 4.84154748916626s\n",
            "       labels\n",
            "13145       0\n",
            "13146       0\n",
            "13147       0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.97      0.72      1918\n",
            "           1       0.99      0.88      0.93     11230\n",
            "\n",
            "    accuracy                           0.89     13148\n",
            "   macro avg       0.79      0.92      0.83     13148\n",
            "weighted avg       0.93      0.89      0.90     13148\n",
            "\n",
            "[[1856   62]\n",
            " [1361 9869]]\n",
            "Accuracy of the model : 0.892\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "pred_lbl = (model.predict(Xtest1)> 0.5).astype(\"int32\")\n",
        "stop = time.time()\n",
        "print(f\"classification time: {stop - start}s\")\n",
        "pd.DataFrame(pred_lbl).to_csv('cdar7.csv', index=False)\n",
        "\n",
        "#pd.DataFrame(Xtest).to_csv('xtest.csv', index=False) # numpy array to CSV\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"cdar7.csv\", encoding='latin-1')\n",
        "messages.columns = [\"labels\"]\n",
        "print (messages.tail(3))\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(labels1, messages[\"labels\"]))\n",
        "print(metrics.confusion_matrix(labels1, messages[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(labels1, messages[\"labels\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr2E9wwGAr3T"
      },
      "outputs": [],
      "source": [
        "# !pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYvrbYA55y6n"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cmoJqEO5y6n"
      },
      "source": [
        "__Using and updating pre-trained embeddings__\n",
        "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
        "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSotteKR5y6n"
      },
      "source": [
        "1. __Load `Word2Vec` Pre-trained Word Embedding__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2nKQfJ4SjTL"
      },
      "outputs": [],
      "source": [
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj5TNHXoSqnu"
      },
      "outputs": [],
      "source": [
        "# !gunzip ./GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4reO_BB5Ar3U"
      },
      "outputs": [],
      "source": [
        "# !pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVM2MjIM5y6n"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMfAK97F5y6o",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyK9HJN05y6o"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk55yUtN5y6o"
      },
      "outputs": [],
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud5lEgbT5y6o",
        "outputId": "36a25f6c-24a4-49fe-a0d0-adadf3c8b1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 19806 words present from 51609 training vocabulary in the set of pre-trained word vector\n"
          ]
        }
      ],
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz7TuXtU5y6p"
      },
      "source": [
        "2. __Define a `pretrained_embedding_layer` function__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDsa8YB25y6p"
      },
      "outputs": [],
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XaW-qwv5y6p"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1GlfQQb5y6q"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfP7mMPz5y6q"
      },
      "source": [
        "## TCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktc3tr1t5y6q"
      },
      "outputs": [],
      "source": [
        "def tcn_model(kernel_size = 3, activation='relu', input_dim = None, \n",
        "                   output_dim=300, max_length = None, emb_matrix = None):\n",
        "    \n",
        "    inp = Input( shape=(max_length,))\n",
        "    x = Embedding(input_dim=input_dim, \n",
        "                  output_dim=output_dim, \n",
        "                  input_length=max_length,\n",
        "                  # Assign the embedding weight with word2vec embedding marix\n",
        "                  weights = [emb_matrix],\n",
        "                  # Set the weight to be not trainable (static)\n",
        "                  trainable = False)(inp)\n",
        "    \n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    \n",
        "    x = TCN(128,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn1')(x)\n",
        "    x = TCN(64,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn2')(x)\n",
        "    \n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    \n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(16, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
        "\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGFFtc_D5y6q"
      },
      "outputs": [],
      "source": [
        "model_0 = tcn_model(input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300)) \n",
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdqLMJGI5y6r"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAWY9Bx25y6r"
      },
      "outputs": [],
      "source": [
        "# class myCallback(tf.keras.callbacks.Callback):\n",
        "#     # Overide the method on_epoch_end() for our benefit\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         if (logs.get('accuracy') >= 0.9):\n",
        "#             print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "#             self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=10, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cek6Ceaa5y6r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "activation = 'relu'\n",
        "print('Loading embedding statistics . . .')\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "print('Done!')\n",
        "kernel_size = [3]\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "exp = 0\n",
        "\n",
        "train_x = sentences\n",
        "train_y = labels\n",
        "# Turn the labels into a numpy array\n",
        "train_y = np.array(train_y)\n",
        "# encode data using\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "# Define the input shape\n",
        "model2 = tcn_model(kernel_size, activation, input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model2.fit(Xtrain, train_y, batch_size=50, epochs=7, verbose=1, callbacks=[callbacks])\n",
        "stop = time.time()\n",
        "print(f\"Training time per fold: {stop - start}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMaw8sOFAr3f",
        "outputId": "40fa529d-aec4-40a0-9551-33ea7b1e11d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13148, 2)\n",
            "download whichapp for whatsapp friends to see your friends apps and also save battery by  you have  friend waiting httpbitlyogmdkv\n",
            "Messages length 13148\n"
          ]
        }
      ],
      "source": [
        "corpus1 = pd.read_csv(\"super23_test.csv\", encoding='latin-1')\n",
        "# corpus1 = pd.read_excel(\"test_punny.xlsx\")\n",
        "corpus1.columns =[\"sentence\", \"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus1.label = corpus1.label.astype(int)\n",
        "print(corpus1.shape)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "print(sentences1[0])\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "\n",
        "test_x1 = sentences1\n",
        "test_y1 = labels1\n",
        "\n",
        "test_sequences1 = tokenizer.texts_to_sequences(test_x1)\n",
        "Xtest1 = pad_sequences(test_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(\"Messages length\", len(test_x1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndeijZuWAr3f"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "pred_lbl = (model2.predict(Xtest1)> 0.5).astype(\"int32\")\n",
        "stop = time.time()\n",
        "print(f\"classification time: {stop - start}s\")\n",
        "pd.DataFrame(pred_lbl).to_csv('cdar5.csv', index=False)\n",
        "\n",
        "#pd.DataFrame(Xtest).to_csv('xtest.csv', index=False) # numpy array to CSV\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"cdar5.csv\", encoding='latin-1')\n",
        "messages.columns = [\"labels\"]\n",
        "print (messages.tail(3))\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(labels1, messages[\"labels\"]))\n",
        "print(metrics.confusion_matrix(labels1, messages[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(labels1, messages[\"labels\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN9oxddG5y6s"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uaAbj4i5y6t"
      },
      "source": [
        "* In this part,  we will fine tune the embeddings while training (dynamic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NnegbKL5y6t"
      },
      "source": [
        "## TCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB9a5QLK5y6t"
      },
      "outputs": [],
      "source": [
        "def define_model_3(kernel_size = 3, activation='relu', input_dim = None, \n",
        "                   output_dim=300, max_length = None, emb_matrix = None):\n",
        "    \n",
        "    inp = Input( shape=(max_length,))\n",
        "    x = Embedding(input_dim=input_dim, \n",
        "                  output_dim=output_dim, \n",
        "                  input_length=max_length,\n",
        "                  # Assign the embedding weight with word2vec embedding marix\n",
        "                  weights = [emb_matrix],\n",
        "                  # Set the weight to be not trainable (static)\n",
        "                  trainable = True)(inp)\n",
        "    \n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    \n",
        "    x = TCN(128,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn1')(x)\n",
        "    x = TCN(64,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn2')(x)\n",
        "    \n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    \n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(16, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
        "\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1iLSCmz5y6t"
      },
      "outputs": [],
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZoYTgs-5y6u"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcGqHkww5y6u"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=10, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPljt1Sd5y6u",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "activations = ['relu']\n",
        "print('Loading embedding statistics . . .')\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "print('Done!')\n",
        "kernel_sizes = [3]\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "exp = 0\n",
        "\n",
        "train_x = sentences\n",
        "train_y = labels\n",
        "# Turn the labels into a numpy array\n",
        "train_y = np.array(train_y)\n",
        "# encode data using\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "\n",
        "# Define the input shape\n",
        "model3 = define_model_3(kernel_size, activation, input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model3.fit(Xtrain, train_y, batch_size=50, epochs=7, verbose=1, callbacks=[callbacks])\n",
        "stop = time.time()\n",
        "print(f\"Training time per fold: {stop - start}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "armdMcYs5y6v"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwxgeIloAr3l",
        "outputId": "7370ced7-2915-43db-ef23-1f424a8e38aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(37615, 2)\n",
            "YOU HAVE WON! As a valued Vodafone customer our computer has picked YOU to win a ?150 prize. To collect is easy. Just call 09061743386\n",
            "Messages length 37615\n"
          ]
        }
      ],
      "source": [
        "corpus1 = pd.read_csv(\"super23_legacy.csv\", encoding='latin-1')\n",
        "# corpus1 = pd.read_excel(\"test_punny.xlsx\")\n",
        "corpus1.columns =[\"sentence\", \"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus1.label = corpus1.label.astype(int)\n",
        "print(corpus1.shape)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "print(sentences1[0])\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "\n",
        "test_x1 = sentences1\n",
        "test_y1 = labels1\n",
        "\n",
        "test_sequences1 = tokenizer.texts_to_sequences(test_x1)\n",
        "Xtest1 = pad_sequences(test_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(\"Messages length\", len(test_x1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UtwSDhvyAr3l",
        "outputId": "ef4282be-9ce5-4a77-98a7-c685b779a3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1176/1176 [==============================] - 12s 10ms/step\n",
            "classification time: 12.64870023727417s\n",
            "       labels\n",
            "37612       0\n",
            "37613       0\n",
            "37614       0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89     36925\n",
            "           1       0.08      0.97      0.15       690\n",
            "\n",
            "    accuracy                           0.81     37615\n",
            "   macro avg       0.54      0.88      0.52     37615\n",
            "weighted avg       0.98      0.81      0.88     37615\n",
            "\n",
            "[[29655  7270]\n",
            " [   24   666]]\n",
            "Accuracy of the model : 0.806\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "pred_lbl = (model3.predict(Xtest1)> 0.5).astype(\"int32\")\n",
        "stop = time.time()\n",
        "print(f\"classification time: {stop - start}s\")\n",
        "pd.DataFrame(pred_lbl).to_csv('cdar6.csv', index=False)\n",
        "\n",
        "#pd.DataFrame(Xtest).to_csv('xtest.csv', index=False) # numpy array to CSV\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"cdar6.csv\", encoding='latin-1')\n",
        "messages.columns = [\"labels\"]\n",
        "print (messages.tail(3))\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(labels1, messages[\"labels\"]))\n",
        "print(metrics.confusion_matrix(labels1, messages[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(labels1, messages[\"labels\"])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dlc1",
      "language": "python",
      "name": "dlc1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}