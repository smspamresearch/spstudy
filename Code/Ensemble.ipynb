{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGWbllI38rV5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU')) #\"/GPU:0\": The first GPU of machine that is visible to TensorFlow.\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "# tf.test.is_built_with_cuda()\n",
        "# tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns+\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"Loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upGgUXw-Bf0z"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oyUyy9sBf0z"
      },
      "outputs": [],
      "source": [
        "# tf.debugging.set_log_device_placement(True) \n",
        "corpus = pd.read_csv(\"super23_train.csv\", encoding='latin-1')\n",
        "corpus.columns =[\"sentence\",\"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus.label = corpus.label.astype(int)\n",
        "print(corpus.shape)\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8wohJqCBf01"
      },
      "outputs": [],
      "source": [
        "corpus.info()\n",
        "corpus.groupby(by='label').count()\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "# sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqFT9_RtBf04"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute the max length of sequence\n",
        "def max_length(sequences):\n",
        "    '''\n",
        "    input:\n",
        "        sequences: a 2D list of integer sequences\n",
        "    output:\n",
        "        max_length: the max length of the sequences\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJYG8ngFBf05"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "\n",
        "print(\"Example of sentence: \", sentences[4])\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "print('Into a sequence of int:', training_sequences[4])\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print('Into a padded sequence:', training_padded[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCHWWv1uBf06"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index\n",
        "# See the first 10 words in the vocabulary\n",
        "for i, word in enumerate(word_index):\n",
        "    print(word, word_index.get(word))\n",
        "    if i==9:\n",
        "        break\n",
        "vocab_size = len(word_index)+1\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vERtRM4lBf07"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dropout, MaxPool1D, Flatten, Dense, Bidirectional, GRU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaxACTQBf08"
      },
      "outputs": [],
      "source": [
        "def define_model(filters = 100, kernel_size = 3, activation='relu', input_dim = None, output_dim=300, max_length = None ):\n",
        "  \n",
        "    # Channel 1\n",
        "    input1 = Input(shape=(max_length,))\n",
        "    embeddding1 = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_length)(input1)\n",
        "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', \n",
        "                   kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n",
        "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    drop1 = Dropout(0.5)(flat1)\n",
        "    dense1 = Dense(10, activation='relu')(drop1)\n",
        "    drop1 = Dropout(0.5)(dense1)\n",
        "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
        "    \n",
        "    # Channel 2\n",
        "    input2 = Input(shape=(max_length,))\n",
        "    embeddding2 = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_length, mask_zero=True)(input2)\n",
        "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
        "    drop2 = Dropout(0.5)(gru2)\n",
        "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
        "    \n",
        "    # Merge\n",
        "    merged = concatenate([out1, out2])\n",
        "    \n",
        "    # Interpretation\n",
        "    outputs = Dense(1, activation='sigmoid')(merged)\n",
        "    model11 = Model(inputs=[input1, input2], outputs=outputs)\n",
        "    \n",
        "    # Compile\n",
        "    model11.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRcQ72cmBf08"
      },
      "outputs": [],
      "source": [
        "model_0 = define_model( input_dim=1000, max_length=100)\n",
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec7KrGfVBf08"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=7, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOnOdHxpBf09"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-QqdfpE8rWI"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "activation = ['relu']\n",
        "filters = 100\n",
        "kernel_size = [3]\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "print(sentences[0])\n",
        "\n",
        "train_x = sentences\n",
        "train_y = labels\n",
        "\n",
        "# Turn the labels into a numpy array\n",
        "train_y = np.array(train_y)\n",
        "\n",
        "# encode data using\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "            \n",
        "# Define the input shape\n",
        "model11 = define_model(filters, kernel_size, activation, input_dim=vocab_size, max_length=max_len)\n",
        "\n",
        "# Train the model and initialize test accuracy with 0\n",
        "acc = 0\n",
        "start = time.time()\n",
        "\n",
        "model11.fit(x=[Xtrain, Xtrain], y = train_y, batch_size=50, epochs=10, verbose=1, callbacks=[callbacks])\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")           \n",
        "# mean_acc = np.array(acc_list).mean()\n",
        "# parameters = [activation, kernel_size]\n",
        "# entries = parameters + acc_list + [mean_acc]\n",
        "\n",
        "# temp = pd.DataFrame([entries], columns=columns)\n",
        "# record = record.append(temp, ignore_index=True)\n",
        "# print()\n",
        "# print(record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGvE_cOd8rWJ",
        "outputId": "b8f28b36-ff44-4501-c5cb-5e2bce7d91c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13148, 2)\n",
            "download whichapp for whatsapp friends to see your friends apps and also save battery by  you have  friend waiting httpbitlyogmdkv\n",
            "Messages length 13148\n"
          ]
        }
      ],
      "source": [
        "corpus1 = pd.read_csv(\"super23_test.csv\", encoding='latin-1')\n",
        "# corpus1 = pd.read_excel(\"test_punny.xlsx\")\n",
        "corpus1.columns =[\"sentence\", \"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus1.label = corpus1.label.astype(int)\n",
        "print(corpus1.shape)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "print(sentences1[0])\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "\n",
        "test_x1 = sentences1\n",
        "test_y1 = labels1\n",
        "\n",
        "test_sequences1 = tokenizer.texts_to_sequences(test_x1)\n",
        "Xtest1 = pad_sequences(test_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(\"Messages length\", len(test_x1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRpHGhNU8rWJ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "pred_lbl = (model11.predict([Xtest1,Xtest1])> 0.5).astype(\"int32\")\n",
        "stop = time.time()\n",
        "print(f\"classification time: {stop - start}s\")\n",
        "pd.DataFrame(pred_lbl).to_csv('cda7.csv', index=False)\n",
        "\n",
        "#pd.DataFrame(Xtest).to_csv('xtest.csv', index=False) # numpy array to CSV\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"cda7.csv\", encoding='latin-1')\n",
        "messages.columns = [\"labels\"]\n",
        "print (messages.tail(3))\n",
        "\n",
        "from sklearn import metrics\n",
        "# print(metrics.classification_report(labels1, messages[\"labels\"]))\n",
        "print(metrics.confusion_matrix(labels1, messages[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(labels1, messages[\"labels\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCn71GkMBf0_"
      },
      "source": [
        "# Model 2: Word2Vec Static"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJPna2u_8rWL"
      },
      "outputs": [],
      "source": [
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXDmdSOf8rWL"
      },
      "outputs": [],
      "source": [
        "!wget -c \"https://figshare.com/ndownloader/files/10798046\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n2Jyow-8rWM"
      },
      "outputs": [],
      "source": [
        "!gunzip ./GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVIek0ua8rWM"
      },
      "outputs": [],
      "source": [
        "# !pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e4_o-FeBf1A"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohdv_5AzBf1A",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Access the dense vector value for the word 'handsome'\n",
        "# word2vec.word_vec('handsome') # 0.11376953\n",
        "word2vec.word_vec('cool') # 1.64062500e-01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXXL-t90Bf1A"
      },
      "source": [
        "2. __Check number of training words present in Word2Vec__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS-QQLRwBf1A"
      },
      "outputs": [],
      "source": [
        "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    count = 0\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            count+=1\n",
        "            \n",
        "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bN0CKu_Bf1A"
      },
      "outputs": [],
      "source": [
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "training_words_in_word2vector(word2vec, word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_LfUztTBf1B"
      },
      "outputs": [],
      "source": [
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()\n",
        "print('emb_mean: ', emb_mean)\n",
        "print('emb_std: ', emb_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAoD0A8cBf1B"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2021)\n",
        "    \n",
        "    # adding 1 to fit Keras embedding (requirement)\n",
        "    vocab_size = len(word_to_index) + 1\n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxMfR3QVBf1B"
      },
      "outputs": [],
      "source": [
        "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
        "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
        "em_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41WL8X8CBf1C"
      },
      "outputs": [],
      "source": [
        "def ensemble_CNN_BiGRU(filters = 100, kernel_size = 3, activation='relu', \n",
        "                   input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n",
        "  \n",
        "    # Channel 1D CNN\n",
        "    input1 = Input(shape=(max_length,))\n",
        "    embeddding1 = Embedding(input_dim=input_dim, \n",
        "                            output_dim=output_dim, \n",
        "                            input_length=max_length, \n",
        "                            input_shape=(max_length, ),\n",
        "                            # Assign the embedding weight with word2vec embedding marix\n",
        "                            weights = [emb_matrix],\n",
        "                            # Set the weight to be not trainable (static)\n",
        "                            trainable = False)(input1)\n",
        "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', \n",
        "                   kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n",
        "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    drop1 = Dropout(0.5)(flat1)\n",
        "    dense1 = Dense(10, activation='relu')(drop1)\n",
        "    drop1 = Dropout(0.5)(dense1)\n",
        "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
        "    \n",
        "    # Channel BiGRU\n",
        "    input2 = Input(shape=(max_length,))\n",
        "    embeddding2 = Embedding(input_dim=input_dim, \n",
        "                            output_dim=output_dim, \n",
        "                            input_length=max_length, \n",
        "                            input_shape=(max_length, ),\n",
        "                            # Assign the embedding weight with word2vec embedding marix\n",
        "                            weights = [emb_matrix],\n",
        "                            # Set the weight to be not trainable (static)\n",
        "                            trainable = False,\n",
        "                            mask_zero=True)(input2)\n",
        "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
        "    drop2 = Dropout(0.5)(gru2)\n",
        "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
        "    \n",
        "    # Merge\n",
        "    merged = concatenate([out1, out2])\n",
        "    \n",
        "    # Interpretation\n",
        "    outputs = Dense(1, activation='sigmoid')(merged)\n",
        "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
        "    \n",
        "    # Compile\n",
        "    model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYcwxCXCBf1C"
      },
      "outputs": [],
      "source": [
        "model_0 = ensemble_CNN_BiGRU(input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUdtQaeWBf1C"
      },
      "source": [
        "## Train and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBAzfHs1Bf1C"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') >= 0.9):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=7, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlthymnYBf1C",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "activation = ['relu']\n",
        "filters = 100\n",
        "kernel_size = [3]\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "train_x = sentences\n",
        "train_y = labels\n",
        "\n",
        "# Turn the labels into a numpy array\n",
        "train_y = np.array(train_y)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "            \n",
        "# Define the input shape\n",
        "model = ensemble_CNN_BiGRU(filters, kernel_size, activation, input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "# Train the model and initialize test accuracy with 0\n",
        "acc = 0\n",
        "start = time.time()             \n",
        "# Train the model\n",
        "model.fit(x=[Xtrain, Xtrain], y = train_y, batch_size=50, epochs=10, verbose=1, callbacks=[callbacks])\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")           \n",
        "# mean_acc = np.array(acc_list).mean()\n",
        "# parameters = [activation, kernel_size]\n",
        "# entries = parameters + acc_list + [mean_acc]\n",
        "\n",
        "# temp = pd.DataFrame([entries], columns=columns)\n",
        "# record = record.append(temp, ignore_index=True)\n",
        "# print()\n",
        "# print(record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7YLwTbQ8rWR",
        "outputId": "2645e80c-e6b3-4464-af1f-988b1cd46df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13148, 2)\n",
            "download whichapp for whatsapp friends to see your friends apps and also save battery by  you have  friend waiting httpbitlyogmdkv\n",
            "Messages length 13148\n"
          ]
        }
      ],
      "source": [
        "corpus1 = pd.read_csv(\"super23_test.csv\", encoding='latin-1')\n",
        "# corpus1 = pd.read_excel(\"test_punny.xlsx\")\n",
        "corpus1.columns =[\"sentence\", \"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus1.label = corpus1.label.astype(int)\n",
        "print(corpus1.shape)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "print(sentences1[0])\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "\n",
        "test_x1 = sentences1\n",
        "test_y1 = labels1\n",
        "\n",
        "test_sequences1 = tokenizer.texts_to_sequences(test_x1)\n",
        "Xtest1 = pad_sequences(test_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print(\"Messages length\", len(test_x1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMcgqIFG8rWS",
        "outputId": "48d5aafe-a621-4082-f007-289c434f5c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "411/411 [==============================] - 5s 6ms/step\n",
            "classification time: 4.811775207519531s\n",
            "       labels\n",
            "13145       0\n",
            "13146       0\n",
            "13147       0\n",
            "[[ 1900    18]\n",
            " [  938 10292]]\n",
            "Accuracy of the model : 0.927\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "pred_lbl = (model.predict([Xtest1,Xtest1])> 0.5).astype(\"int32\")\n",
        "stop = time.time()\n",
        "print(f\"classification time: {stop - start}s\")\n",
        "pd.DataFrame(pred_lbl).to_csv('cda8.csv', index=False)\n",
        "\n",
        "#pd.DataFrame(Xtest).to_csv('xtest.csv', index=False) # numpy array to CSV\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"cda8.csv\", encoding='latin-1')\n",
        "messages.columns = [\"labels\"]\n",
        "print (messages.tail(3))\n",
        "\n",
        "from sklearn import metrics\n",
        "# print(metrics.classification_report(labels1, messages[\"labels\"]))\n",
        "print(metrics.confusion_matrix(labels1, messages[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(labels1, messages[\"labels\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPGaQOGu8rWS"
      },
      "outputs": [],
      "source": [
        "# pred_lbl = (model.predict([Xtest,Xtest]) > 0.5).astype(\"int32\")\n",
        "# pd.DataFrame(pred_lbl).to_csv('output1.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idW_Gs_38rWS"
      },
      "outputs": [],
      "source": [
        "#pd.DataFrame(Xtest).to_csv('xtest.csv', index=False) # numpy array to CSV\n",
        "# import pandas as pd\n",
        "# messages = pd.read_csv(\"output1.csv\", encoding='latin-1')\n",
        "# messages.columns = [\"labels\"]\n",
        "# print (messages.tail(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pthw29vy8rWT"
      },
      "outputs": [],
      "source": [
        "# from sklearn import metrics\n",
        "# print(metrics.classification_report(test_y, messages[\"labels\"]))\n",
        "# print(metrics.confusion_matrix(test_y, messages[\"labels\"]))\n",
        "\n",
        "# # Printing the Overall Accuracy of the model\n",
        "# print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(test_y, messages[\"labels\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRhqhGy5Bf1D"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lwS49V4Bf1D"
      },
      "outputs": [],
      "source": [
        "# record2.sort_values(by='AVG', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc0Xg6AZBf1D"
      },
      "outputs": [],
      "source": [
        "# record2[['Activation', 'AVG']].groupby(by='Activation').max().sort_values(by='AVG', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKAbej2IBf1E"
      },
      "source": [
        "# Model 3: Word2Vec - Dynamic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBOrOr9QBf1E"
      },
      "outputs": [],
      "source": [
        "def define_model_3(filters = 100, kernel_size = 3, activation='relu', \n",
        "                   input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n",
        "  \n",
        "    # Channel 1\n",
        "    input1 = Input(shape=(max_length,))\n",
        "    embeddding1 = Embedding(input_dim=input_dim, \n",
        "                            output_dim=output_dim, \n",
        "                            input_length=max_length, \n",
        "                            input_shape=(max_length, ),\n",
        "                            # Assign the embedding weight with word2vec embedding marix\n",
        "                            weights = [emb_matrix],\n",
        "                            # Set the weight to be not trainable (static)\n",
        "                            trainable = True)(input1)\n",
        "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', \n",
        "                   kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n",
        "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    drop1 = Dropout(0.5)(flat1)\n",
        "    dense1 = Dense(10, activation='relu')(drop1)\n",
        "    drop1 = Dropout(0.5)(dense1)\n",
        "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
        "    \n",
        "    # Channel 2\n",
        "    input2 = Input(shape=(max_length,))\n",
        "    embeddding2 = Embedding(input_dim=input_dim, \n",
        "                            output_dim=output_dim, \n",
        "                            input_length=max_length, \n",
        "                            input_shape=(max_length, ),\n",
        "                            # Assign the embedding weight with word2vec embedding marix\n",
        "                            weights = [emb_matrix],\n",
        "                            # Set the weight to be not trainable (static)\n",
        "                            trainable = True,\n",
        "                            mask_zero=True)(input2)\n",
        "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
        "    drop2 = Dropout(0.5)(gru2)\n",
        "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
        "    \n",
        "    # Merge\n",
        "    merged = concatenate([out1, out2])\n",
        "    \n",
        "    # Interpretation\n",
        "    outputs = Dense(1, activation='sigmoid')(merged)\n",
        "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
        "    \n",
        "    # Compile\n",
        "    model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4PVaim-Bf1E"
      },
      "outputs": [],
      "source": [
        "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
        "# model_0.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-6ZvA6XBf1E"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTqHWGRwBf1F"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    # Overide the method on_epoch_end() for our benefit\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy') > 0.93):\n",
        "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=10, verbose=2, \n",
        "                                             mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqmD25PCBf1F",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Parameter Initialization\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<UNK>\"\n",
        "activations = ['relu']\n",
        "filters = 100\n",
        "kernel_size = [3]\n",
        "emb_mean = emb_mean\n",
        "emb_std = emb_std\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
        "\n",
        "train_x = sentences\n",
        "train_y = labels\n",
        "\n",
        "# Turn the labels into a numpy array\n",
        "train_y = np.array(train_y)\n",
        "\n",
        "# encode data using\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "\n",
        "max_len = max_length(training_sequences)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
        "            \n",
        "# Define the input shape\n",
        "model3 = define_model_3(filters, kernel_size, activation, input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
        "\n",
        "# Train the model and initialize test accuracy with 0\n",
        "acc = 0\n",
        "start = time.time()             \n",
        "# Train the model\n",
        "model3.fit(x=[Xtrain, Xtrain], y = train_y, batch_size=50, epochs=10, verbose=1, callbacks=[callbacks])\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")           \n",
        "# mean_acc = np.array(acc_list).mean()\n",
        "# parameters = [activation, kernel_size]\n",
        "# entries = parameters + acc_list + [mean_acc]\n",
        "\n",
        "# temp = pd.DataFrame([entries], columns=columns)\n",
        "# record = record.append(temp, ignore_index=True)\n",
        "# print()\n",
        "# print(record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRiyorirBf1F"
      },
      "source": [
        "## Test The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY3PQDjs8rWX",
        "outputId": "4887cade-e15d-4eb1-edce-bc1b1ae013be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13148, 2)\n",
            "download whichapp for whatsapp friends to see your friends apps and also save battery by  you have  friend waiting httpbitlyogmdkv\n",
            "Messages length 13148\n"
          ]
        }
      ],
      "source": [
        "corpus1 = pd.read_csv(\"super23_test.csv\", encoding='latin-1')\n",
        "# corpus1 = pd.read_excel(\"test_punny.xlsx\")\n",
        "corpus1.columns =[\"sentence\", \"label\"]\n",
        "# corpus['label']= corpus['label'].map({'ham': 0, 'spam': 1})\n",
        "corpus1.label = corpus1.label.astype(int)\n",
        "print(corpus1.shape)\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "print(sentences1[0])\n",
        "\n",
        "# Separate the sentences and the labels\n",
        "sentences1, labels1 = list(corpus1.sentence), list(corpus1.label)\n",
        "\n",
        "test_x1 = sentences1\n",
        "test_y1 = labels1\n",
        "\n",
        "test_sequences1 = tokenizer.texts_to_sequences(test_x1)\n",
        "Xtest1 = pad_sequences(test_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "print(\"Messages length\", len(test_x1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA9mXezi8rWY",
        "outputId": "e24eab01-f2bc-4b91-c0ac-59b085f9ce74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "411/411 [==============================] - 5s 6ms/step\n",
            "classification time: 4.729171276092529s\n",
            "       labels\n",
            "13145       0\n",
            "13146       0\n",
            "13147       0\n",
            "[[ 1819    99]\n",
            " [ 1004 10226]]\n",
            "Accuracy of the model : 0.916\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "pred_lbl = (model3.predict([Xtest1,Xtest1])> 0.5).astype(\"int32\")\n",
        "stop = time.time()\n",
        "print(f\"classification time: {stop - start}s\")\n",
        "pd.DataFrame(pred_lbl).to_csv('cda9.csv', index=False)\n",
        "\n",
        "#pd.DataFrame(Xtest).to_csv('xtest.csv', index=False) # numpy array to CSV\n",
        "import pandas as pd\n",
        "messages = pd.read_csv(\"cda9.csv\", encoding='latin-1')\n",
        "messages.columns = [\"labels\"]\n",
        "print (messages.tail(3))\n",
        "\n",
        "from sklearn import metrics\n",
        "# print(metrics.classification_report(labels1, messages[\"labels\"]))\n",
        "print(metrics.confusion_matrix(labels1, messages[\"labels\"]))\n",
        "\n",
        "# Printing the Overall Accuracy of the model\n",
        "print(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(labels1, messages[\"labels\"])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8tWvR83BBf1C",
        "kUdtQaeWBf1C",
        "CRhqhGy5Bf1D",
        "Ey9UUiBgBf1E",
        "y-6ZvA6XBf1E",
        "PRiyorirBf1F"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dlc1",
      "language": "python",
      "name": "dlc1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}