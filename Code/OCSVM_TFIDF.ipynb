{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmlcQn-zWfjx"
      },
      "outputs": [],
      "source": [
        "# Load packages\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.utils import shuffle\n",
        "# from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def text_preprocess(mess):\n",
        "    # Check characters to see if they are in punctuation\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\n",
        "\n",
        "    # Join the characters again to form the string.\n",
        "    nopunc = ''.join(nopunc)\n",
        "    nopunc = nopunc.lower()\n",
        "\n",
        "    # Now just remove any stopwords and non alphabets\n",
        "    nostop = [word for word in nopunc.split() if word.lower() not in stopwords.words('english') and word.isalpha()]\n",
        "    return nostop\n",
        "\n",
        "# load dataset\n",
        "msgs_all = pd.read_csv('super23.csv',encoding='latin-1')\n",
        "# msgs_all = pd.read_excel('punny.xlsx')\n",
        "# msgs_all = msgs_all.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
        "msgs_all.columns = [\"label\",\"message\"]\n",
        "\n",
        "# change category labels\n",
        "msgs_all['label'] = msgs_all['label'].map({1:1,0:-1})\n",
        "# create a new dataset with only spam category data\n",
        "# msgs_spam = msgs_all.loc[0:53854, :]\n",
        "# print(len(msgs_spam))\n",
        "msgs_all[\"message\"] = msgs_all[\"message\"].apply(text_preprocess)\n",
        "msgs_all[\"message\"] = msgs_all[\"message\"].agg(lambda x: ' '.join(map(str, x)))\n",
        "print (msgs_all.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msgs_spam = msgs_all.loc[37816:65736, :] #0:42627, 10658:53286, 0:52812\n",
        "msgs_spam = msgs_spam[msgs_spam['label'] == 1]\n",
        "print(msgs_spam.shape)\n",
        "\n",
        "# msgs_all[\"message\"] = msgs_all[\"message\"].apply(text_preprocess)\n",
        "# msgs_all[\"message\"] = msgs_all[\"message\"].agg(lambda x: ' '.join(map(str, x)))\n",
        "\n",
        "# create train and test data\n",
        "train_text = msgs_spam['message'].tolist()\n",
        "train_labels = msgs_spam['label'].tolist()\n",
        "\n",
        "# test_text = msgs_all['message'].tolist()\n",
        "# test_labels = msgs_all['label'].tolist()\n",
        "import time\n",
        "start = time.time()\n",
        "# Be considered for Ngram\n",
        "vectorizer = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", ngram_range=(1, 1))\n",
        "# Be considered for BoW\n",
        "# vectorizer = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\")\n",
        "# features = vectorizer.fit_transform(train_text)\n",
        "# print(features.shape)\n",
        "\n",
        "# OneClassSVM algorithm\n",
        "clf = OneClassSVM(nu=0.2, kernel=\"rbf\", gamma=0.1)\n",
        "clf = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
        "# For details on Pipeline, https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
        "\n",
        "# fit OneClassSVM model\n",
        "clf.fit(train_text, train_labels)\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Trg time: {stop - start}s\")"
      ],
      "metadata": {
        "id": "zhpTI3KWWsz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msgs_all = pd.read_csv('super23.csv',encoding='latin-1')\n",
        "# msgs_all = pd.read_excel('punny.xlsx')\n",
        "# msgs_all = msgs_all.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
        "msgs_all.columns = [\"label\",\"message\"]\n",
        "msgs_all.head(3)"
      ],
      "metadata": {
        "id": "HJNrFdNtHSoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msgs_test = msgs_all.loc[0:37815, :] # 42627:53286, 0:10657, 52812:53286\n",
        "\n",
        "msgs_test[\"message\"] = msgs_test[\"message\"].apply(text_preprocess)\n",
        "msgs_test[\"message\"] = msgs_test[\"message\"].agg(lambda x: ' '.join(map(str, x)))\n",
        "\n",
        "test_text = msgs_test['message'].tolist()\n",
        "test_labels = msgs_test['label'].tolist()\n",
        "import time\n",
        "start = time.time()\n",
        "# validate OneClassSVM model with test set\n",
        "preds_test = clf.predict(test_text)\n",
        "# print(preds_test)\n",
        "# stop = time.time()\n",
        "# print(f\"Testing time: {stop - start}s\")\n",
        "\n",
        "results = confusion_matrix(test_labels, preds_test)\n",
        "print('Confusion Matrix :', results)\n",
        "# print('Precision:', precision_score(test_labels, preds_test))\n",
        "# print('Recall:', recall_score(test_labels, preds_test))\n",
        "# print('F1-Score :',f1_score(test_labels, preds_test))\n",
        "print('Accuracy Score :',accuracy_score(test_labels, preds_test))\n",
        "# print('Report : ', classification_report(test_labels, preds_test))"
      ],
      "metadata": {
        "id": "C4Gz4n6JWuJ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}